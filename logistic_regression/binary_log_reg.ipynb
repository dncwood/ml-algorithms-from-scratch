{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea8ff70",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression Theory\n",
    "Logistic regression is a supervised learning algorithm used to classify an observation into one of two classes (binary classification) or into one of multiple classes (multiclass classification). It is primarily used to solve classification problems.\n",
    "\n",
    "While in a regression problem we aim to answer questions like *\"How much?\"* or *\"How many?\"*, in a classification problem we focus on questions such as *\"Which category?\"*. For example, a binary classification question might be *\"Is this email spam or not spam?\"*, while a multiclass classification question could be *\"Which digit (0-9) does this image represent?\"*\n",
    "\n",
    "In this notebook, we focus on **binary logistic regression**.\n",
    "\n",
    "The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation.\n",
    "\n",
    "The classifier takes a feature vector $\\mathbf{x} \\in \\mathbb{R}^n$ as input and outputs a scalar value $y \\in \\{0, 1\\}$.\n",
    "\n",
    "The output $y$ of the classifier can be:\n",
    "- $y = 1$: indicating that the observation belongs to the target class,\n",
    "- $y = 0$: indicating that the observation does **not** belong to the target class.\n",
    "\n",
    "### Model\n",
    "So why is it called *logistic regression* and not *logistic classification*?\n",
    "\n",
    "This is because logistic regression, by itself, is not a classification algorithm. Rather, it is a regression model that estimates the probability of class membership. Specifically, it models this probability as a transformation of a linear combination of the input features.\n",
    "\n",
    "In binary logistic regression, our goal is to estimate the probability that a given observation belongs to the positive class — that is, we want to compute:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "While linear regression outputs a continuous value $y$ for a given input $\\mathbf{x}$, logistic regression outputs a continuous value representing the probability $P(y = 1 \\mid \\mathbf{x})$ — a value between 0 and 1.\n",
    "\n",
    "In other words, logistic regression performs regression on the probability of a **categorical outcome**.\n",
    "\n",
    "Logistic regression becomes a **classification** algorithm only when it is paired with a **decision rule**. This decision rule typically converts the predicted probability into a class label.\n",
    "\n",
    "Therefore, logistic regression is actually a type of **generalized linear model (GLM)**. It uses the same fundamental formula as linear regression.\n",
    "\n",
    "Recall that in linear regression, we predict the continuous output variable $y$ using a weighted sum of the input features. Here we'll use the notation $z$ for the resulting variable:\n",
    "\n",
    "$$\n",
    "z = \\sum_j w_j x_j + b\n",
    "$$\n",
    "\n",
    "In logistic regression, we follow the same approach, but then we apply an additional transformation: we pass $z$ through the **sigmoid function**.\n",
    "\n",
    "#### Sigmoid function\n",
    "\n",
    "Note that in a linear equation, there is nothing that forces the output $z$ to be a valid probability — that is, a value between 0 and 1.\n",
    "\n",
    "To transform this output into a valid probability, we pass $z$ through a special function called the **sigmoid function**, denoted by $\\sigma(z)$. This function is also known as the **logistic function**, which is where logistic regression gets its name.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{e^z + 1}\n",
    "$$\n",
    "\n",
    "The sigmoid function has several useful properties:\n",
    "- It maps any real-valued number into the interval (0, 1), which is perfect for interpreting the output as a probability.\n",
    "- It is nearly linear around 0 but flattens toward 0 and 1, which helps squash extreme values and reduce the effect of outliers.\n",
    "- It is differentiable, which is crucial for training the model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../images/sigmoid_plot.png\" alt=\"Sigmoid Function\" width=\"40%\"/>\n",
    "</div>\n",
    "\n",
    "To model a probability over two classes, we must ensure that the probabilities of the two possible outcomes $P(y=1)$ and $P(y=0)$ sum to 1. Using the sigmoid function, we define:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y = 0 \\mid \\mathbf{x}) = 1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "The input to the sigmoid function, $z = \\mathbf{w}^T \\mathbf{x} + b$, is often referred to as the **logit**. This name comes from the fact that the sigmoid function is the inverse of the **logit function**, which is defined as the natural logarithm (usually defined as log in statistics and ML) of the odds ratio. Let $p = \\sigma(z)$ and the odds ratio $\\frac{p}{1 - p}$. The logit function is then:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "$$\n",
    "\n",
    "Let's solve for z:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "1 + e^{-z} &= \\frac{1}{p} \\\\\n",
    "e^{-z} &= \\frac{1}{p} - 1 = \\frac{1 - p}{p} \\\\\n",
    "e^z &= \\frac{p}{1 - p} \\\\\n",
    "\\log(e^z) &= \\log\\left(\\frac{p}{1 - p}\\right) \\\\\n",
    "z &= \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $z = \\mathbf{w}^T \\mathbf{x} + b$, we have:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "So, by using the sigmoid function to convert $z$ (which can range from $-\\infty$ to $\\infty$) into a probability, we are implicitly interpreting $z$ as the **log-odds** of the outcome. That’s why $z$ is called the logit.\n",
    "\n",
    "A logistic regression model is part of the GLM family not because the estimated probability is a linear function, but because the **logit** of the estimated probability is a linear function of the input parameters.\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "Although logistic regression is fundamentally a regression algorithm — estimating the probability $P(y = 1 \\mid \\mathbf{x})$ — we can still use it for classification by applying a **decision rule** to the output probability.\n",
    "\n",
    "For a given input $\\mathbf{x}$, we classify the observation as class 1 if the predicted probability is greater than 0.5, and as class 0 otherwise. The value 0.5 is known as the **decision boundary** or **threshold**.\n",
    "\n",
    "Formally, the decision rule is:\n",
    "\n",
    "$$\n",
    "decision(\\mathbf{x}) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } P(y = 1 \\mid \\mathbf{x}) > 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This simple thresholding turns the continuous probability output of the model into a discrete class label.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Recall that in linear regression we used **mean squared error (MSE)** as the cost function, which was motivated by **maximum likelihood estimation (MLE)** under the assumption of a **Gaussian distribution**.\n",
    "\n",
    "In binary logistic regression, we also use MLE, but since the output is a probability over two classes, we model the target using a **Bernoulli distribution**.\n",
    "\n",
    "The **Bernoulli distribution** is a probability distribution over a single binary random variable. It is controlled by a single parameter $\\phi \\in [0, 1]$, which represents the probability of the random variable being equal to 1:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x = 1) &= \\phi \\\\\n",
    "P(x = 0) &= 1 - \\phi \\\\\n",
    "P(x) &= \\phi^x (1 - \\phi)^{1 - x}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Adapting this to our case — where the predicted probability $\\hat{y}$ is given by the sigmoid function $\\sigma(z)$ — the probability of observing a label $y \\in \\{0, 1\\}$ becomes:\n",
    "\n",
    "$$\n",
    "P(y) = \\hat{y}^y (1 - \\hat{y})^{1 - y}\n",
    "$$\n",
    "\n",
    "This expression forms the basis of the loss function we will minimize during training.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "To understand why MLE is a commonly used principle in machine learning, let us first define the concept in detail.\n",
    "\n",
    "Suppose we have a dataset $X = \\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(m)}\\}$ consisting of $m$ examples, which we assume to be drawn independently from the true, but unknown, data-generating distribution $p_{\\text{data}}(\\mathbf{x})$.\n",
    "\n",
    "Now consider a model distribution $p_{\\text{model}}(\\mathbf{x};\\theta)$ — a parametric family of probability distributions over the same sample space, indexed by a parameter vector $\\theta$. In other words, $p_{\\text{model}}(\\mathbf{x}; \\theta)$ maps any configuration $\\mathbf{x}$ to a real number estimating the true probability $p_{\\text{data}}(\\mathbf{x})$.\n",
    "\n",
    "To explain this concept in an abstract way, consider a famous aphorism attributed to George Box:\n",
    "\n",
    "> \"All models are wrong, but some are useful.\"\n",
    "\n",
    "This means:\n",
    "- \"All models are wrong\" — because every model is a simplification of reality, and hence cannot be perfectly accurate (in this context, it cannot fully represent $p_{\\text{data}}(\\mathbf{x})$).\n",
    "- \"But some are useful\" — because simplifications can still be powerful tools for prediction, explanation, and understanding (this is what $p_{\\text{model}}(\\mathbf{x};\\theta)$ attempts).\n",
    "\n",
    "Now, the **maximum likelihood estimate** of the parameters $\\theta$ is defined as:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{ML}} = \\underset{\\theta}{\\arg\\max} \\; p_{\\text{model}}(X;\\theta) = \\underset{\\theta}{\\arg\\max} \\; \\prod_{i=1}^m p_{\\text{model}}(\\mathbf{x}^{(i)};\\theta)\n",
    "$$\n",
    "\n",
    "To simplify this expression, we take the logarithm of the product:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{ML}} = \\underset{\\theta}{\\arg\\max} \\; \\sum_{i=1}^m \\log p_{\\text{model}}(\\mathbf{x}^{(i)};\\theta)\n",
    "$$\n",
    "\n",
    "Since rescaling does not affect the $\\arg\\max$, we can divide by $m$ to obtain the expectation with respect to the empirical distribution $\\hat{p}_{data}$ defined by the training data:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{ML}} = \\underset{\\theta}{\\arg\\max} \\; \\frac{1}{m} \\sum_{i=1}^m \\log p_{\\text{model}}(\\mathbf{x}^{(i)};\\theta)\n",
    "$$\n",
    "\n",
    "One way to interpret MLE is through **KL divergence**, a measure of how one probability distribution diverges from another. In this case, we want the model distribution to match the empirical distribution $\\hat{p}_{\\text{data}}$ defined by the training set:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(\\hat{p}_{\\text{data}} \\; || \\; p_{\\text{model}}) = \\frac{1}{m} \\sum_{i=1}^m \\log \\hat{p}_{\\text{data}}(\\mathbf{x}^{(i)}) - \\log p_{\\text{model}}(\\mathbf{x}^{(i)})\n",
    "$$\n",
    "\n",
    "Since $\\log \\hat{p}_{\\text{data}}(\\mathbf{x}^{(i)})$ is independent of the model parameters, minimizing the KL divergence reduces to minimizing:\n",
    "\n",
    "$$\n",
    "- \\frac{1}{m} \\sum_{i=1}^m \\log p_{\\text{model}}(\\mathbf{x}^{(i)})\n",
    "$$\n",
    "\n",
    "Minimizing this expression is the same as **maximizing the log-likelihood**, since minimizing a negative quantity is equivalent to maximizing its positive counterpart.\n",
    "\n",
    "Matter of fact in practice, instead of maximizing the log-likelihood, we often **minimize the negative log-likelihood (NLL)**. Doing so is exactly equivalent to minimizing the cross-entropy between the empirical distribution $\\hat{p}_{\\text{data}}$ and the model distribution $p_{\\text{model}}$ as shown above. This is why, in many machine learning contexts, the cost function when using MLE is often referred to as the **cross-entropy loss**.\n",
    "\n",
    "Thus, maximum likelihood estimation can be interpreted as an attempt to make the model distribution match the empirical data distribution as closely as possible. Ideally, we would want to match the true distribution $p_{\\text{data}}$, but since we don't have access to it, we rely on $\\hat{p}_{\\text{data}}$ from the training data.\n",
    "\n",
    "MLE is also favored because of its **consistency**, meaning that as the number of training examples $m \\rightarrow \\infty$, the estimated parameter $\\theta_{\\text{ML}}$ converges to the true parameter value.\n",
    "\n",
    "And while there are other consistent estimators, no other consistent estimator is as **statistically efficient** as MLE — meaning that, for a fixed number of examples, MLE typically achieves the lowest possible generalization error.\n",
    "\n",
    "#### Negative Log-Likelihood Cost Function\n",
    "\n",
    "The cost function in **binary logistic regression**, as motivated by **maximum likelihood estimation (MLE)**, becomes the **negative log-likelihood** for a **Bernoulli distribution**.\n",
    "\n",
    "This is generalized to our case where our goal is to estimate a conditional probability $P(\\mathbf{y} \\mid X; \\mathbf{w}, b)$ in order to predict $\\mathbf{y}$ given $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}, b) &= \\frac{1}{m} \\prod_{i=1}^{m} P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\mathbf{w}, b) = \\frac{1}{m} \\prod_{i=1}^{m} \\hat{y}^{(i)y^{(i)}} (1 - \\hat{y}^{(i)})^{1 - y^{(i)}} \\\\\n",
    "J(\\mathbf{w}, b) &= \\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right) \\\\\n",
    "J(\\mathbf{w}, b) &= -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The **loss function** for a single training example is then:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = - \\left( y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "According to MLE, to find the **optimal parameters** $\\mathbf{w}$ and $b$, we **minimize the loss function**:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\mathbf{w}, b} \\; L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "For updating the parameters during training, we use the **mini-batch stochastic gradient descent** algorithm. This means that instead of processing the entire dataset at once, we split it into mini-batches $B = \\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(m')}\\}$ of fixed size $m'$ and compute the gradients using only the data in each mini-batch. \n",
    "\n",
    "$$\n",
    "g = \\frac{1}{m'} \\sum_{i=1}^{m'} \\nabla L(\\mathbf{w}, b)^{(i)}\n",
    "$$\n",
    "\n",
    "The gradient of the loss with respect to the weights $\\mathbf{w}$ and bias $b$ for an example $(\\mathbf{x}, y)$ is computed via chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{w}, b)}{\\partial (\\mathbf{w}, b)} &= \\frac{\\partial L(\\mathbf{w}, b)}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial (\\mathbf{w}, b)} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial L(\\mathbf{w}, b)}{\\partial \\hat{y}} &= - (\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}) = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{\\partial}{\\partial z} \\frac{e^{z}}{1+e^{z}} \\\\\n",
    "&=\\frac{e^{z}(1+e^{z})-e^{z}e^{z}}{(1+e^{z})^2} \\\\\n",
    "&=\\frac{e^{z}}{(1+e^{z})^2} \\\\\n",
    "&=\\frac{e^{z}}{1+e^{z}} \\frac{1}{1+e^{z}} \\\\\n",
    "&=\\frac{e^{z}}{1+e^{z}} (1-\\frac{e^{z}}{1+e^{z}}) \\\\\n",
    "&=\\hat{y} (1-\\hat{y})\n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial \\mathbf{w}} &= \\mathbf{x} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial b} &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} L^{(i)} = (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{b} L^{(i)} = \\hat{y}^{(i)} - y^{(i)}\n",
    "$$\n",
    "\n",
    "To compute the average gradients over the mini-batch, we take:\n",
    "\n",
    "$$\n",
    "g_{\\mathbf{w}} = \\frac{1}{m'} \\sum_{i=1}^{m'} (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_b = \\frac{1}{m'} \\sum_{i=1}^{m'} (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "Note that while the surface level of the equation looks the same as in linear regression the definition of $\\hat{y}$ is different. Matter of fact this is a shared property of generalized linear models.\n",
    "\n",
    "To update the model parameters, we multiply the gradients by the learning rate $\\eta$ and subtract the result from the current parameter values:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\eta \\, g_{\\mathbf{w}}, \\quad b := b - \\eta \\, g_b\n",
    "$$\n",
    "\n",
    "Note that the subtraction is due to our goal of minimizing the negative log-likelihood. If we were instead maximizing the log-likelihood directly, we would add the gradient update to the current parameter values:\n",
    "\n",
    "If you want to explore gradient descent in more detail, refer to the last notebook.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "After training the model for a fixed number of epochs—or until some other stopping criterion is met on the training set—we need to evaluate its performance on the test set.\n",
    "\n",
    "In linear regression, we used metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and the coefficient of determination $R^2$. While these metrics provide intuitive insights in regression tasks, they are not as suitable for logistic regression, which deals with classification.\n",
    "\n",
    "In logistic regression, the most commonly used evaluation metric is **accuracy**.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Accuracy measures the proportion of correct predictions out of all predictions:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} = \\frac{TP + TN}{TP + FN + TN + FP}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP** = True Positives: Correctly predicted positives  \n",
    "- **TN** = True Negatives: Correctly predicted negatives  \n",
    "- **FP** = False Positives: Incorrectly predicted positives  \n",
    "- **FN** = False Negatives: Incorrectly predicted negatives  \n",
    "\n",
    "While accuracy is simple and intuitive, it can be misleading in cases where the data is imbalanced—that is, when one class occurs far more frequently than the other. For example, in a dataset where 95% of samples belong to class 0, a model that always predicts class 0 would achieve 95% accuracy, but would be useless for detecting class 1.\n",
    "\n",
    "Therefore, additional metrics like **precision**, **recall**, and **F1 score** are often used to gain more insight into the model's behavior.\n",
    "\n",
    "#### Precision\n",
    "\n",
    "Precision measures how many of the positively predicted instances are actually positive:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "#### Recall\n",
    "\n",
    "Recall (also known as sensitivity or true positive rate) measures how many of the actual positive instances were correctly predicted:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "The F1 score is the **harmonic mean** of precision and recall. It balances the two and is especially useful when you want to find an optimal balance between precision and recall:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "These metrics help provide a more complete picture of the model’s performance, particularly in situations where the data is imbalanced or when the cost of false positives and false negatives differs significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591198d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Binary logistic regression implementation from scratch\n",
    "\n",
    "Required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b59b5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304ef75",
   "metadata": {},
   "source": [
    "For our implementation of binary logistic regression, we use the **Breast Cancer Wisconsin** dataset. This dataset is well-suited for binary classification tasks because the target variable indicates whether a tumor is **malignant** or **benign**, making it a classic example of a binary outcome.\n",
    "\n",
    "We load the dataset using the `load_breast_cancer` function from the `sklearn.datasets` module. `.DESCR` provides a detailed description of the dataset, including the meaning of each feature and information about the target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7fa5a045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancer_sklearn = load_breast_cancer(as_frame=True)\n",
    "print(cancer_sklearn.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b6120",
   "metadata": {},
   "source": [
    "We load the dataset into a Pandas `DataFrame` to take advantage of Pandas' powerful tools for data manipulation and exploration. This makes it easier to inspect and preprocess the data before feeding it into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "88d798a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "4",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e911041a-5d3b-4787-9233-b93ed22c2186",
       "rows": [
        [
         "mean radius",
         "17.99",
         "20.57",
         "19.69",
         "11.42",
         "20.29"
        ],
        [
         "mean texture",
         "10.38",
         "17.77",
         "21.25",
         "20.38",
         "14.34"
        ],
        [
         "mean perimeter",
         "122.8",
         "132.9",
         "130.0",
         "77.58",
         "135.1"
        ],
        [
         "mean area",
         "1001.0",
         "1326.0",
         "1203.0",
         "386.1",
         "1297.0"
        ],
        [
         "mean smoothness",
         "0.1184",
         "0.08474",
         "0.1096",
         "0.1425",
         "0.1003"
        ],
        [
         "mean compactness",
         "0.2776",
         "0.07864",
         "0.1599",
         "0.2839",
         "0.1328"
        ],
        [
         "mean concavity",
         "0.3001",
         "0.0869",
         "0.1974",
         "0.2414",
         "0.198"
        ],
        [
         "mean concave points",
         "0.1471",
         "0.07017",
         "0.1279",
         "0.1052",
         "0.1043"
        ],
        [
         "mean symmetry",
         "0.2419",
         "0.1812",
         "0.2069",
         "0.2597",
         "0.1809"
        ],
        [
         "mean fractal dimension",
         "0.07871",
         "0.05667",
         "0.05999",
         "0.09744",
         "0.05883"
        ],
        [
         "radius error",
         "1.095",
         "0.5435",
         "0.7456",
         "0.4956",
         "0.7572"
        ],
        [
         "texture error",
         "0.9053",
         "0.7339",
         "0.7869",
         "1.156",
         "0.7813"
        ],
        [
         "perimeter error",
         "8.589",
         "3.398",
         "4.585",
         "3.445",
         "5.438"
        ],
        [
         "area error",
         "153.4",
         "74.08",
         "94.03",
         "27.23",
         "94.44"
        ],
        [
         "smoothness error",
         "0.006399",
         "0.005225",
         "0.00615",
         "0.00911",
         "0.01149"
        ],
        [
         "compactness error",
         "0.04904",
         "0.01308",
         "0.04006",
         "0.07458",
         "0.02461"
        ],
        [
         "concavity error",
         "0.05373",
         "0.0186",
         "0.03832",
         "0.05661",
         "0.05688"
        ],
        [
         "concave points error",
         "0.01587",
         "0.0134",
         "0.02058",
         "0.01867",
         "0.01885"
        ],
        [
         "symmetry error",
         "0.03003",
         "0.01389",
         "0.0225",
         "0.05963",
         "0.01756"
        ],
        [
         "fractal dimension error",
         "0.006193",
         "0.003532",
         "0.004571",
         "0.009208",
         "0.005115"
        ],
        [
         "worst radius",
         "25.38",
         "24.99",
         "23.57",
         "14.91",
         "22.54"
        ],
        [
         "worst texture",
         "17.33",
         "23.41",
         "25.53",
         "26.5",
         "16.67"
        ],
        [
         "worst perimeter",
         "184.6",
         "158.8",
         "152.5",
         "98.87",
         "152.2"
        ],
        [
         "worst area",
         "2019.0",
         "1956.0",
         "1709.0",
         "567.7",
         "1575.0"
        ],
        [
         "worst smoothness",
         "0.1622",
         "0.1238",
         "0.1444",
         "0.2098",
         "0.1374"
        ],
        [
         "worst compactness",
         "0.6656",
         "0.1866",
         "0.4245",
         "0.8663",
         "0.205"
        ],
        [
         "worst concavity",
         "0.7119",
         "0.2416",
         "0.4504",
         "0.6869",
         "0.4"
        ],
        [
         "worst concave points",
         "0.2654",
         "0.186",
         "0.243",
         "0.2575",
         "0.1625"
        ],
        [
         "worst symmetry",
         "0.4601",
         "0.275",
         "0.3613",
         "0.6638",
         "0.2364"
        ],
        [
         "worst fractal dimension",
         "0.1189",
         "0.08902",
         "0.08758",
         "0.173",
         "0.07678"
        ],
        [
         "target",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 31
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>17.990000</td>\n",
       "      <td>20.570000</td>\n",
       "      <td>19.690000</td>\n",
       "      <td>11.420000</td>\n",
       "      <td>20.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>10.380000</td>\n",
       "      <td>17.770000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>20.380000</td>\n",
       "      <td>14.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>122.800000</td>\n",
       "      <td>132.900000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>77.580000</td>\n",
       "      <td>135.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>1326.000000</td>\n",
       "      <td>1203.000000</td>\n",
       "      <td>386.100000</td>\n",
       "      <td>1297.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.084740</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.078640</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>0.078710</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>0.058830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>1.095000</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.757200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>1.156000</td>\n",
       "      <td>0.781300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>8.589000</td>\n",
       "      <td>3.398000</td>\n",
       "      <td>4.585000</td>\n",
       "      <td>3.445000</td>\n",
       "      <td>5.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>153.400000</td>\n",
       "      <td>74.080000</td>\n",
       "      <td>94.030000</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>94.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.011490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>0.074580</td>\n",
       "      <td>0.024610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>0.053730</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.056610</td>\n",
       "      <td>0.056880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.018850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.017560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.005115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>25.380000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>23.570000</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>22.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>17.330000</td>\n",
       "      <td>23.410000</td>\n",
       "      <td>25.530000</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>16.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>184.600000</td>\n",
       "      <td>158.800000</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>98.870000</td>\n",
       "      <td>152.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>1956.000000</td>\n",
       "      <td>1709.000000</td>\n",
       "      <td>567.700000</td>\n",
       "      <td>1575.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.089020</td>\n",
       "      <td>0.087580</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.076780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0            1            2           3  \\\n",
       "mean radius                17.990000    20.570000    19.690000   11.420000   \n",
       "mean texture               10.380000    17.770000    21.250000   20.380000   \n",
       "mean perimeter            122.800000   132.900000   130.000000   77.580000   \n",
       "mean area                1001.000000  1326.000000  1203.000000  386.100000   \n",
       "mean smoothness             0.118400     0.084740     0.109600    0.142500   \n",
       "mean compactness            0.277600     0.078640     0.159900    0.283900   \n",
       "mean concavity              0.300100     0.086900     0.197400    0.241400   \n",
       "mean concave points         0.147100     0.070170     0.127900    0.105200   \n",
       "mean symmetry               0.241900     0.181200     0.206900    0.259700   \n",
       "mean fractal dimension      0.078710     0.056670     0.059990    0.097440   \n",
       "radius error                1.095000     0.543500     0.745600    0.495600   \n",
       "texture error               0.905300     0.733900     0.786900    1.156000   \n",
       "perimeter error             8.589000     3.398000     4.585000    3.445000   \n",
       "area error                153.400000    74.080000    94.030000   27.230000   \n",
       "smoothness error            0.006399     0.005225     0.006150    0.009110   \n",
       "compactness error           0.049040     0.013080     0.040060    0.074580   \n",
       "concavity error             0.053730     0.018600     0.038320    0.056610   \n",
       "concave points error        0.015870     0.013400     0.020580    0.018670   \n",
       "symmetry error              0.030030     0.013890     0.022500    0.059630   \n",
       "fractal dimension error     0.006193     0.003532     0.004571    0.009208   \n",
       "worst radius               25.380000    24.990000    23.570000   14.910000   \n",
       "worst texture              17.330000    23.410000    25.530000   26.500000   \n",
       "worst perimeter           184.600000   158.800000   152.500000   98.870000   \n",
       "worst area               2019.000000  1956.000000  1709.000000  567.700000   \n",
       "worst smoothness            0.162200     0.123800     0.144400    0.209800   \n",
       "worst compactness           0.665600     0.186600     0.424500    0.866300   \n",
       "worst concavity             0.711900     0.241600     0.450400    0.686900   \n",
       "worst concave points        0.265400     0.186000     0.243000    0.257500   \n",
       "worst symmetry              0.460100     0.275000     0.361300    0.663800   \n",
       "worst fractal dimension     0.118900     0.089020     0.087580    0.173000   \n",
       "target                      0.000000     0.000000     0.000000    0.000000   \n",
       "\n",
       "                                   4  \n",
       "mean radius                20.290000  \n",
       "mean texture               14.340000  \n",
       "mean perimeter            135.100000  \n",
       "mean area                1297.000000  \n",
       "mean smoothness             0.100300  \n",
       "mean compactness            0.132800  \n",
       "mean concavity              0.198000  \n",
       "mean concave points         0.104300  \n",
       "mean symmetry               0.180900  \n",
       "mean fractal dimension      0.058830  \n",
       "radius error                0.757200  \n",
       "texture error               0.781300  \n",
       "perimeter error             5.438000  \n",
       "area error                 94.440000  \n",
       "smoothness error            0.011490  \n",
       "compactness error           0.024610  \n",
       "concavity error             0.056880  \n",
       "concave points error        0.018850  \n",
       "symmetry error              0.017560  \n",
       "fractal dimension error     0.005115  \n",
       "worst radius               22.540000  \n",
       "worst texture              16.670000  \n",
       "worst perimeter           152.200000  \n",
       "worst area               1575.000000  \n",
       "worst smoothness            0.137400  \n",
       "worst compactness           0.205000  \n",
       "worst concavity             0.400000  \n",
       "worst concave points        0.162500  \n",
       "worst symmetry              0.236400  \n",
       "worst fractal dimension     0.076780  \n",
       "target                      0.000000  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer: pd.DataFrame= cancer_sklearn.frame\n",
    "cancer.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630975b2",
   "metadata": {},
   "source": [
    "The `info()` method confirms that there are no missing values in the dataset, and that all columns are of appropriate numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b7c6811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e71c8711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "75%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "226531dc-af31-4b1d-9437-051ea1713047",
       "rows": [
        [
         "mean radius",
         "569.0",
         "14.127291739894552",
         "3.5240488262120775",
         "6.981",
         "11.7",
         "13.37",
         "15.78",
         "28.11"
        ],
        [
         "mean texture",
         "569.0",
         "19.289648506151142",
         "4.301035768166949",
         "9.71",
         "16.17",
         "18.84",
         "21.8",
         "39.28"
        ],
        [
         "mean perimeter",
         "569.0",
         "91.96903339191564",
         "24.298981038754906",
         "43.79",
         "75.17",
         "86.24",
         "104.1",
         "188.5"
        ],
        [
         "mean area",
         "569.0",
         "654.8891036906855",
         "351.914129181653",
         "143.5",
         "420.3",
         "551.1",
         "782.7",
         "2501.0"
        ],
        [
         "mean smoothness",
         "569.0",
         "0.0963602811950791",
         "0.01406412813767362",
         "0.05263",
         "0.08637",
         "0.09587",
         "0.1053",
         "0.1634"
        ],
        [
         "mean compactness",
         "569.0",
         "0.10434098418277679",
         "0.052812757932512194",
         "0.01938",
         "0.06492",
         "0.09263",
         "0.1304",
         "0.3454"
        ],
        [
         "mean concavity",
         "569.0",
         "0.0887993158172232",
         "0.07971980870789348",
         "0.0",
         "0.02956",
         "0.06154",
         "0.1307",
         "0.4268"
        ],
        [
         "mean concave points",
         "569.0",
         "0.04891914586994728",
         "0.038802844859153605",
         "0.0",
         "0.02031",
         "0.0335",
         "0.074",
         "0.2012"
        ],
        [
         "mean symmetry",
         "569.0",
         "0.18116186291739894",
         "0.027414281336035715",
         "0.106",
         "0.1619",
         "0.1792",
         "0.1957",
         "0.304"
        ],
        [
         "mean fractal dimension",
         "569.0",
         "0.06279760984182776",
         "0.007060362795084459",
         "0.04996",
         "0.0577",
         "0.06154",
         "0.06612",
         "0.09744"
        ],
        [
         "radius error",
         "569.0",
         "0.40517205623901575",
         "0.2773127329861039",
         "0.1115",
         "0.2324",
         "0.3242",
         "0.4789",
         "2.873"
        ],
        [
         "texture error",
         "569.0",
         "1.2168534270650264",
         "0.5516483926172023",
         "0.3602",
         "0.8339",
         "1.108",
         "1.474",
         "4.885"
        ],
        [
         "perimeter error",
         "569.0",
         "2.8660592267135327",
         "2.0218545540421076",
         "0.757",
         "1.606",
         "2.287",
         "3.357",
         "21.98"
        ],
        [
         "area error",
         "569.0",
         "40.337079086116",
         "45.49100551613181",
         "6.802",
         "17.85",
         "24.53",
         "45.19",
         "542.2"
        ],
        [
         "smoothness error",
         "569.0",
         "0.007040978910369069",
         "0.0030025179438390656",
         "0.001713",
         "0.005169",
         "0.00638",
         "0.008146",
         "0.03113"
        ],
        [
         "compactness error",
         "569.0",
         "0.025478138840070295",
         "0.017908179325677388",
         "0.002252",
         "0.01308",
         "0.02045",
         "0.03245",
         "0.1354"
        ],
        [
         "concavity error",
         "569.0",
         "0.03189371634446397",
         "0.03018606032298841",
         "0.0",
         "0.01509",
         "0.02589",
         "0.04205",
         "0.396"
        ],
        [
         "concave points error",
         "569.0",
         "0.011796137082601054",
         "0.006170285174046869",
         "0.0",
         "0.007638",
         "0.01093",
         "0.01471",
         "0.05279"
        ],
        [
         "symmetry error",
         "569.0",
         "0.02054229876977153",
         "0.008266371528798399",
         "0.007882",
         "0.01516",
         "0.01873",
         "0.02348",
         "0.07895"
        ],
        [
         "fractal dimension error",
         "569.0",
         "0.0037949038664323374",
         "0.002646070967089195",
         "0.0008948",
         "0.002248",
         "0.003187",
         "0.004558",
         "0.02984"
        ],
        [
         "worst radius",
         "569.0",
         "16.269189806678387",
         "4.833241580469323",
         "7.93",
         "13.01",
         "14.97",
         "18.79",
         "36.04"
        ],
        [
         "worst texture",
         "569.0",
         "25.677223198594024",
         "6.146257623038319",
         "12.02",
         "21.08",
         "25.41",
         "29.72",
         "49.54"
        ],
        [
         "worst perimeter",
         "569.0",
         "107.26121265377857",
         "33.602542269036356",
         "50.41",
         "84.11",
         "97.66",
         "125.4",
         "251.2"
        ],
        [
         "worst area",
         "569.0",
         "880.5831282952548",
         "569.356992669949",
         "185.2",
         "515.3",
         "686.5",
         "1084.0",
         "4254.0"
        ],
        [
         "worst smoothness",
         "569.0",
         "0.13236859402460457",
         "0.022832429404835465",
         "0.07117",
         "0.1166",
         "0.1313",
         "0.146",
         "0.2226"
        ],
        [
         "worst compactness",
         "569.0",
         "0.25426504393673116",
         "0.157336488913742",
         "0.02729",
         "0.1472",
         "0.2119",
         "0.3391",
         "1.058"
        ],
        [
         "worst concavity",
         "569.0",
         "0.27218848330404216",
         "0.2086242806081323",
         "0.0",
         "0.1145",
         "0.2267",
         "0.3829",
         "1.252"
        ],
        [
         "worst concave points",
         "569.0",
         "0.11460622319859401",
         "0.06573234119594207",
         "0.0",
         "0.06493",
         "0.09993",
         "0.1614",
         "0.291"
        ],
        [
         "worst symmetry",
         "569.0",
         "0.2900755711775044",
         "0.061867467537518685",
         "0.1565",
         "0.2504",
         "0.2822",
         "0.3179",
         "0.6638"
        ],
        [
         "worst fractal dimension",
         "569.0",
         "0.0839458172231986",
         "0.018061267348893986",
         "0.05504",
         "0.07146",
         "0.08004",
         "0.09208",
         "0.2075"
        ],
        [
         "target",
         "569.0",
         "0.6274165202108963",
         "0.48391795640316865",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 31
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "target                   569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "target                     0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa5da1",
   "metadata": {},
   "source": [
    "Next, we split the dataset into features and target. The target variable, `target`, indicates whether the tumor is malignant or benign. All other columns serve as input features for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e0165741",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: pd.DataFrame = cancer.drop(\"target\", axis=1)\n",
    "y: pd.Series = cancer[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9d38d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: np.ndarray = X.to_numpy()\n",
    "y: np.ndarray = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e458464",
   "metadata": {},
   "source": [
    "Before creating and training our model, we define several hyperparameters that control the process. We can also set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e7884941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "eta = 0.1\n",
    "val_split = 0.1\n",
    "split_ratio = 0.75\n",
    "feature_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed34e4",
   "metadata": {},
   "source": [
    "Before training the model, we shuffle the dataset, split it into training and test sets, and normalize the feature values based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "11a1d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    shuffle_indices = np.random.permutation(len(X))\n",
    "    X, y = X[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9460068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3931c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X: np.ndarray, y: np.ndarray, split_ratio: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    split_size = int(len(X) * split_ratio)\n",
    "    X_train = X[:split_size]\n",
    "    y_train = y[:split_size]\n",
    "    X_test = X[split_size:]\n",
    "    y_test = y[split_size:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "624f8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_dataset(X, y, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9adb62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    def fit_transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        self.min_data = min(data)\n",
    "        self.max_data = max(data)\n",
    "        \n",
    "        return (data - self.min_data) / (self.max_data - self.min_data)\n",
    "    \n",
    "    def fit(self, data: np.ndarray) -> np.ndarray:\n",
    "        return (data - self.min_data) / (self.max_data - self.min_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "270be73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(X_train: np.ndarray, X_test: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X_train_transpose = X_train.transpose()\n",
    "    X_test_transpose = X_test.transpose()\n",
    "    \n",
    "    for i in range(len(X_train_transpose)):\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_transpose[i, :] = scaler.fit_transform(X_train_transpose[i, :])\n",
    "        X_test_transpose[i, :] = scaler.fit(X_test_transpose[i, :])\n",
    "        \n",
    "    return X_train_transpose.transpose(), X_test_transpose.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "99170a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = min_max_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207f037",
   "metadata": {},
   "source": [
    "To train our logistic regression model, we define a loss class which implements the negative log-likelihood loss assuming a Bernoulli distribution and its gradient with respect to the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ad506850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def mean_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        losses: np.ndarray = self(y_true, y_pred)\n",
    "        mean_loss = np.sum(losses) / len(losses)\n",
    "\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "21477ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(Loss):   \n",
    "    def __call__(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:       \n",
    "        losses = -1 * (y_true * (np.log(y_pred)) + (1 - y_true) * (np.log(1 - y_pred)))\n",
    "        return losses\n",
    "    \n",
    "    def gradient(self, y_true: np.ndarray, y_pred: np.ndarray, X: np.ndarray) -> tuple[np.ndarray, float]:      \n",
    "        grad_w = np.dot((y_pred - y_true), X) / len(X)\n",
    "        grad_b = np.sum((y_pred - y_true)) / len(X)\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e81e92",
   "metadata": {},
   "source": [
    "We now define a modular implementation of binary logistic regression.\n",
    "\n",
    "- The `BLogRModel` class handles parameter initialization and forward computation using the sigmoid function. In the `BLogRModel`, we implement the sigmoid function in a numerically stable way to avoid overflow issues during exponentiation. When `z` is a large negative number, `np.exp(-z)` can overflow. To handle this, we compute the sigmoid differently depending on the sign of `z`.\n",
    "- The `BLogRModule` class includes the training loop (`fit`), prediction (`predict`), and evaluation (`evaluate`) methods. It supports batch training, optional validation, and reports common metrics like accuracy, precision, recall, and F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c3e22268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLogRModule:\n",
    "    def __init__(self):\n",
    "        self.loss = NLLLoss()\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int, batch_size: int = 1, eta: float = 0.01, val_split: float = None) -> tuple[list[float], list[float]]:\n",
    "        if val_split:\n",
    "                split_size = int(len(X) * val_split)\n",
    "                X_val = X[:split_size]\n",
    "                y_val = y[:split_size]\n",
    "                X = X[split_size:]\n",
    "                y = y[split_size:]\n",
    "    \n",
    "        batches = [(X[i : i + batch_size], y[i : i + batch_size]) for i in range(0, len(X), batch_size)]\n",
    "        \n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for X_batch, y_true in batches:\n",
    "                y_pred = self(X_batch)\n",
    "                loss += self.loss.mean_loss(y_true, y_pred)\n",
    "                grad_w, grad_b = self.loss.gradient(y_true, y_pred, X_batch)\n",
    "                self.w -= eta * grad_w\n",
    "                self.b -= eta * grad_b\n",
    "\n",
    "            if val_split:\n",
    "                y_pred_val = self(X_val)\n",
    "                val_loss = self.loss.mean_loss(y_val, y_pred_val)\n",
    "                print(f\"Epoch {i}: Loss {loss/len(batches)}, Val-Loss {val_loss}\")\n",
    "                val_losses.append(val_loss)\n",
    "            else:\n",
    "                print(f\"Epoch {i}: Loss {loss/len(batches)}\")\n",
    "                \n",
    "            losses.append(loss/len(batches))\n",
    "\n",
    "        return losses, val_losses\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        y_pred = self(X)\n",
    "        \n",
    "        return np.round(y_pred)\n",
    "    \n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray) -> tuple[float, float, float, float]:\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + fn + tn + fp)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "               \n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLogRModel(BLogRModule):\n",
    "    def __init__(self, feature_dim: int):\n",
    "        super().__init__()\n",
    "        self.w: np.ndarray = np.random.normal(size=(feature_dim))\n",
    "        self.b: np.ndarray = np.random.normal(size=1)\n",
    "        \n",
    "    def _positive_sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _negative_sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(z)\n",
    "        \n",
    "        return exp / (exp + 1)\n",
    "    \n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        positive = z >= 0\n",
    "        negative = ~positive\n",
    "\n",
    "        result = np.empty_like(z, dtype=float)\n",
    "        result[positive] = self._positive_sigmoid(z[positive])\n",
    "        result[negative] = self._negative_sigmoid(z[negative])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def __call__(self, X: np.ndarray) -> float:\n",
    "        z = self.b + np.dot(X, self.w)\n",
    "        y_pred = self.sigmoid(z)       \n",
    "        \n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a8f5a",
   "metadata": {},
   "source": [
    "We now instantiate our binary logistic regression model using the number of input features. Training is performed on the training data using the defined hyperparameters. During training, we track and output the loss values for both training and validation (if specified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "04cc1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.5536723533050846, Val-Loss 0.3275855588648005\n",
      "Epoch 1: Loss 0.270984333595188, Val-Loss 0.24230641856317375\n",
      "Epoch 2: Loss 0.21718355969339298, Val-Loss 0.2066393049606716\n",
      "Epoch 3: Loss 0.19125566172955102, Val-Loss 0.18541425411342746\n",
      "Epoch 4: Loss 0.17506905291062436, Val-Loss 0.17069233352908858\n",
      "Epoch 5: Loss 0.1636222518522034, Val-Loss 0.1595772164863669\n",
      "Epoch 6: Loss 0.15491551113843383, Val-Loss 0.15072513217592484\n",
      "Epoch 7: Loss 0.14797170607746638, Val-Loss 0.14341406531029288\n",
      "Epoch 8: Loss 0.1422478886506013, Val-Loss 0.1372153274644164\n",
      "Epoch 9: Loss 0.1374138647674257, Val-Loss 0.13185518847784786\n",
      "Epoch 10: Loss 0.1332549069167928, Val-Loss 0.12714889578488645\n",
      "Epoch 11: Loss 0.12962397446010118, Val-Loss 0.12296617816857686\n",
      "Epoch 12: Loss 0.12641617636837546, Val-Loss 0.1192118744090119\n",
      "Epoch 13: Loss 0.12355417803277309, Val-Loss 0.11581441418843105\n",
      "Epoch 14: Loss 0.12097939577088647, Val-Loss 0.11271863856599548\n",
      "Epoch 15: Loss 0.11864643790833641, Val-Loss 0.1098811456543426\n",
      "Epoch 16: Loss 0.11651945890292303, Val-Loss 0.10726717080623765\n",
      "Epoch 17: Loss 0.11456968889268694, Val-Loss 0.10484843441338484\n",
      "Epoch 18: Loss 0.11277371195691448, Val-Loss 0.10260161964274224\n",
      "Epoch 19: Loss 0.11111223648022284, Val-Loss 0.10050727185563452\n",
      "Epoch 20: Loss 0.10956919798062437, Val-Loss 0.098548987303204\n",
      "Epoch 21: Loss 0.10813109207174, Val-Loss 0.09671280462374433\n",
      "Epoch 22: Loss 0.10678647019670547, Val-Loss 0.09498674130130122\n",
      "Epoch 23: Loss 0.1055255527228648, Val-Loss 0.09336043556263299\n",
      "Epoch 24: Loss 0.10433992812380245, Val-Loss 0.09182486618266958\n",
      "Epoch 25: Loss 0.10322231629289684, Val-Loss 0.09037213068656229\n",
      "Epoch 26: Loss 0.10216638030342527, Val-Loss 0.08899526789920159\n",
      "Epoch 27: Loss 0.10116657523174527, Val-Loss 0.08768811457960235\n",
      "Epoch 28: Loss 0.10021802566234544, Val-Loss 0.08644518854376204\n",
      "Epoch 29: Loss 0.09931642562260758, Val-Loss 0.08526159258422542\n",
      "Epoch 30: Loss 0.0984579562270478, Val-Loss 0.08413293487334021\n",
      "Epoch 31: Loss 0.09763921742793895, Val-Loss 0.08305526254760198\n",
      "Epoch 32: Loss 0.09685717109396486, Val-Loss 0.08202500591940949\n",
      "Epoch 33: Loss 0.09610909325440763, Val-Loss 0.08103893132360149\n",
      "Epoch 34: Loss 0.09539253381108832, Val-Loss 0.08009410103058122\n",
      "Epoch 35: Loss 0.09470528237439653, Val-Loss 0.07918783898196359\n",
      "Epoch 36: Loss 0.09404533915203665, Val-Loss 0.07831770135431769\n",
      "Epoch 37: Loss 0.09341089003025198, Val-Loss 0.07748145115046404\n",
      "Epoch 38: Loss 0.09280028515231198, Val-Loss 0.07667703616951614\n",
      "Epoch 39: Loss 0.09221202042896665, Val-Loss 0.07590256982647996\n",
      "Epoch 40: Loss 0.09164472151857574, Val-Loss 0.07515631438718937\n",
      "Epoch 41: Loss 0.09109712989679718, Val-Loss 0.07443666626023702\n",
      "Epoch 42: Loss 0.09056809070170914, Val-Loss 0.07374214304857728\n",
      "Epoch 43: Loss 0.0900565420934994, Val-Loss 0.07307137211283998\n",
      "Epoch 44: Loss 0.08956150591111117, Val-Loss 0.07242308043853857\n",
      "Epoch 45: Loss 0.08908207944351805, Val-Loss 0.07179608563220323\n",
      "Epoch 46: Loss 0.08861742816224048, Val-Loss 0.07118928789846168\n",
      "Epoch 47: Loss 0.08816677928555, Val-Loss 0.07060166287240506\n",
      "Epoch 48: Loss 0.08772941606453083, Val-Loss 0.07003225520008256\n",
      "Epoch 49: Loss 0.08730467269755611, Val-Loss 0.06948017277541815\n",
      "Epoch 50: Loss 0.0868919297934066, Val-Loss 0.06894458155476124\n",
      "Epoch 51: Loss 0.08649061031470236, Val-Loss 0.06842470088116573\n",
      "Epoch 52: Loss 0.08610017594294728, Val-Loss 0.06791979925966259\n",
      "Epoch 53: Loss 0.08572012381459153, Val-Loss 0.06742919053256827\n",
      "Epoch 54: Loss 0.08534998358439057, Val-Loss 0.06695223041049381\n",
      "Epoch 55: Loss 0.084989314778166, Val-Loss 0.06648831332035236\n",
      "Epoch 56: Loss 0.08463770440203573, Val-Loss 0.06603686953650507\n",
      "Epoch 57: Loss 0.08429476477942958, Val-Loss 0.06559736256533283\n",
      "Epoch 58: Loss 0.08396013159082753, Val-Loss 0.06516928675709799\n",
      "Epoch 59: Loss 0.08363346209428697, Val-Loss 0.06475216512205265\n",
      "Epoch 60: Loss 0.0833144335075085, Val-Loss 0.06434554733042598\n",
      "Epoch 61: Loss 0.08300274153450991, Val-Loss 0.06394900787825197\n",
      "Epoch 62: Loss 0.08269809902198925, Val-Loss 0.06356214440302496\n",
      "Epoch 63: Loss 0.08240023473219768, Val-Loss 0.06318457613493937\n",
      "Epoch 64: Loss 0.08210889222066034, Val-Loss 0.0628159424710232\n",
      "Epoch 65: Loss 0.0818238288084049, Val-Loss 0.06245590166082811\n",
      "Epoch 66: Loss 0.08154481463951216, Val-Loss 0.06210412959354163\n",
      "Epoch 67: Loss 0.08127163181581139, Val-Loss 0.061760318677433604\n",
      "Epoch 68: Loss 0.08100407360143354, Val-Loss 0.061424176803483096\n",
      "Epoch 69: Loss 0.08074194369071123, Val-Loss 0.06109542638585337\n",
      "Epoch 70: Loss 0.08048505553360201, Val-Loss 0.0607738034726133\n",
      "Epoch 71: Loss 0.08023323171341853, Val-Loss 0.06045905692074779\n",
      "Epoch 72: Loss 0.07998630337218252, Val-Loss 0.06015094763008012\n",
      "Epoch 73: Loss 0.07974410967939263, Val-Loss 0.05984924783123929\n",
      "Epoch 74: Loss 0.07950649734042131, Val-Loss 0.05955374042326676\n",
      "Epoch 75: Loss 0.0792733201411242, Val-Loss 0.059264218356863484\n",
      "Epoch 76: Loss 0.07904443852558012, Val-Loss 0.05898048405964775\n",
      "Epoch 77: Loss 0.07881971920417913, Val-Loss 0.05870234890012084\n",
      "Epoch 78: Loss 0.07859903478953374, Val-Loss 0.05842963268733642\n",
      "Epoch 79: Loss 0.0783822634579323, Val-Loss 0.058162163203530196\n",
      "Epoch 80: Loss 0.07816928863426083, Val-Loss 0.05789977576721405\n",
      "Epoch 81: Loss 0.07795999869851196, Val-Loss 0.057642312824442196\n",
      "Epoch 82: Loss 0.07775428671216711, Val-Loss 0.05738962356616617\n",
      "Epoch 83: Loss 0.07755205016289483, Val-Loss 0.05714156356976024\n",
      "Epoch 84: Loss 0.07735319072614492, Val-Loss 0.056897994462963106\n",
      "Epoch 85: Loss 0.07715761404234138, Val-Loss 0.05665878360862905\n",
      "Epoch 86: Loss 0.07696522950849007, Val-Loss 0.056423803808804865\n",
      "Epoch 87: Loss 0.07677595008311992, Val-Loss 0.056192933026776706\n",
      "Epoch 88: Loss 0.07658969210356564, Val-Loss 0.05596605412583665\n",
      "Epoch 89: Loss 0.07640637511468583, Val-Loss 0.05574305462360982\n",
      "Epoch 90: Loss 0.07622592170818243, Val-Loss 0.05552382646088917\n",
      "Epoch 91: Loss 0.07604825737176008, Val-Loss 0.05530826578398901\n",
      "Epoch 92: Loss 0.07587331034742165, Val-Loss 0.05509627273971689\n",
      "Epoch 93: Loss 0.07570101149825678, Val-Loss 0.05488775128212479\n",
      "Epoch 94: Loss 0.0755312941831275, Val-Loss 0.05468260899026642\n",
      "Epoch 95: Loss 0.07536409413870697, Val-Loss 0.05448075689624242\n",
      "Epoch 96: Loss 0.07519934936836559, Val-Loss 0.05428210932287112\n",
      "Epoch 97: Loss 0.07503700003743963, Val-Loss 0.05408658373036737\n",
      "Epoch 98: Loss 0.07487698837445468, Val-Loss 0.053894100571459436\n",
      "Epoch 99: Loss 0.0747192585779055, Val-Loss 0.053704583154412525\n"
     ]
    }
   ],
   "source": [
    "model = BLogRModel(feature_dim)\n",
    "history, val_history = model.fit(X_train, y_train, epochs, batch_size, eta, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb85e",
   "metadata": {},
   "source": [
    "To better understand the training process, we visualize the loss curves over epochs using Seaborn’s `relplot`. This visualization helps monitor convergence and can reveal signs of underfitting or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "881933c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 563.5x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot: sns.FacetGrid = sns.relplot(data=(history, val_history), kind=\"line\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "new_labels = ['Training', 'Validation']\n",
    "for texts, labels in zip(plot._legend.texts, new_labels):\n",
    "    texts.set_text(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331fe5",
   "metadata": {},
   "source": [
    "After training the model, we assess its performance on unseen test data. Using the `predict` method, we generate predictions for the test set. The `evaluate` method then calculates key metrics—accuracy, precision, recall, and F1 score—which provide a comprehensive measure of the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fc32e31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.986013986013986\n",
      "Precision: 0.9787234042553191\n",
      "Recall: 1.0\n",
      "F1 score: 0.989247311827957\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "acc, prec, rec, f1 = model.evaluate(y_pred, y_test)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31430bed",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated a fundamental implementation of linear regression from scratch, applied to a widely used benchmark dataset — the California housing dataset. It also provided a detailed explanation of the theory behind linear regression, helping to connect the mathematical foundations with practical implementation.\n",
    "\n",
    "While the model presented here works well for demonstration purposes, it can be adapted to other datasets as well. However, when doing so, it is important to perform proper exploratory data analysis (EDA) and feature engineering to ensure the model receives meaningful and well-structured input.\n",
    "\n",
    "Finally, this implementation provides a solid foundation that can be extended further. Techniques such as regularization (e.g., L1/L2 penalties), early stopping, or adaptive learning rates could improve generalization and training efficiency, especially when working with more complex or noisy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb06c5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook presented an implementation of binary logistic regression from scratch, using a benchmark dataset — the breast cancer dataset. The theoretical background was thoroughly explained to bridge the gap between the underlying theory and the practical implementation.\n",
    "\n",
    "Beware that in a real production scenario, you would need to perform proper exploratory data analysis (EDA) and feature engineering to ensure the model receives clean and informative input. Additionally, further enhancements such as L1/L2 regularization, adaptive learning rates, or early stopping are crucial for improving model performance, generalization, and training efficiency in more demanding settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637462de",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Box, G. E. P., & Draper, N. R. (1987). *Empirical model-building and response surfaces* (p. 424). Wiley. \n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2025). *Speech and language processing* (Draft of January 12, 2025, Chapter 5: Logistic Regression). Retrieved from https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press. Retrieved from https://www.deeplearningbook.org/\n",
    "\n",
    "Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2022). *Dive into deep learning*. Retrieved from https://d2l.ai/chapter_linear-classification/softmax-regression.html\n",
    "\n",
    "Ng, A. (2018). *Lecture 3 - Locally Weighted & Logistic Regression* [Video]. YouTube. Stanford CS229: Machine Learning. Retrieved from \n",
    "https://www.youtube.com/watch?v=het9HFqo1TQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=3\n",
    "\n",
    "Ng, A. (2018). *Lecture 4 - Perceptron & Generalized Linear Model* [Video]. YouTube. Stanford CS229: Machine Learning. Retrieved from\n",
    "https://www.youtube.com/watch?v=iZTeva0WSTQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
