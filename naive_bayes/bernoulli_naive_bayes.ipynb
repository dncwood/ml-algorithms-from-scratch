{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458b9699",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes Theory\n",
    "\n",
    "Naive Bayes is a probabilistic classifier that relies on supervised learning. Similar to logistic regression, it aims to assign new observations, represented as feature vectors $\\mathbf{x} \\in \\mathbb{R}^n$, to one of a set of discrete classes. In the case of binary classification, the output is a scalar value $y \\in \\{0, 1\\}$.\n",
    "\n",
    "A key distinction between logistic regression and Naive Bayes lies in their modeling approaches: logistic regression is a **discriminative** classifier, while Naive Bayes is a **generative** classifier. These two paradigms represent different frameworks for building machine learning models.\n",
    "\n",
    "To illustrate this difference, consider the task of distinguishing images of dogs from images of cats:\n",
    "\n",
    "- A **generative model** attempts to learn the underlying distribution of each class. In principle, such a model could generate a sample (e.g., draw a dog). Given a new image, the model evaluates which class distribution—dog or cat—better explains the observation, and assigns the corresponding label.\n",
    "- A **discriminative model**, on the other hand, focuses only on finding boundaries between classes. For instance, if all dogs in the dataset wear collars and none of the cats do, the model may rely solely on this feature to separate the classes. In this case, the model has little to say about the actual characteristics of cats or dogs beyond this distinction.\n",
    "\n",
    "There are three common variants of Naive Bayes classifiers, which differ in how they represent features:\n",
    "\n",
    "- **Bernoulli Naive Bayes**: works with binary features, modeling the presence or absence of a feature.\n",
    "- **Multinomial Naive Bayes**: models features as frequency counts (e.g., word counts in text classification).\n",
    "- **Gaussian Naive Bayes**: assumes continuous features that follow a Gaussian distribution.\n",
    "\n",
    "In this notebook, we focus on the **Bernoulli Naive Bayes classifier**. While the specific feature representations differ among the variants, the fundamental concepts of Naive Bayes apply to all of them.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "The Naive Bayes classifier derives its name from **Bayes' Theorem**, combined with a simplifying (or \"naive\") assumption about feature independence. Before applying it to classification, let us first recall Bayes' Theorem, a fundamental result in probability theory:\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H)P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $H$ represents a hypothesis, and $E$ represents the observed evidence.\n",
    "- $P(H|E)$ is the **posterior probability** of the hypothesis $H$ given the evidence $E$.\n",
    "- $P(E|H)$ is the **likelihood** of $H$ give a fixed $E$, i.e., the probability of observing $E$ given that $H$ is true.\n",
    "- $P(H)$ is the **prior probability** of the hypothesis $H$, independent of any evidence.\n",
    "- $P(E)$ is the **marginal probability** of the evidence $E$.\n",
    "\n",
    "The essential idea is that evidence does not determine beliefs in isolation. Instead, it updates prior beliefs. In other words, to compute the posterior probability $P(H|E)$, both the prior $P(H)$ and the likelihood $P(E|H)$ must be taken into account.\n",
    "\n",
    "### Model\n",
    "\n",
    "To understand the Naive Bayes classifier, let us consider the classic **spam vs. ham** example. Given an email, the task is to classify it as spam or not spam (ham).\n",
    "\n",
    "The first step is to represent an email as a feature vector $\\mathbf{x}$. We construct an $n$-dimensional dictionary of the most frequent words in the dataset. In **Bernoulli Naive Bayes**, the features are binary, so each entry in $\\mathbf{x} \\in \\{0, 1\\}^n$ indicates the **presence** ($1$) or **absence** ($0$) of a particular word in the email.\n",
    "\n",
    "The classifier assigns the email to the class $\\hat{y}$ with the maximum posterior probability given the feature vector:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max} \\; p(y|\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Using Bayes’ Theorem, we can rewrite the posterior probability as:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{x}) = \\frac{p(\\mathbf{x}|y)p(y)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Thus, to compute $p(y|\\mathbf{x})$, we need to model the **likelihood** $p(\\mathbf{x}|y)$ and the **prior probability** $p(y)$. Since the marginal probability $p(\\mathbf{x})$ does not depend on $y$ and remains constant across classes, it can be ignored in this case.\n",
    "\n",
    "A direct modeling of $p(\\mathbf{x}|y)$ would require estimating $2^n$ parameters, since each of the $n$ features can take binary values. Applying the chain rule, the likelihood can be written as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y) = p(x_1, x_2, \\dots, x_n|y) = p(x_1|x_2, \\dots, x_n, y) \\, p(x_2|x_3, \\dots, x_n, y) \\dots p(x_n|y)\n",
    "$$\n",
    "\n",
    "With a large enough $n$ this escalates and the formulation becomes computationally intractable. Naive Bayes resolves this by making the **conditional independence assumption**: the features are assumed to be independent given $y$. Under this assumption:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|y) = p(x_1, x_2, \\dots, x_n|y) = \\prod_{j=1}^n p(x_j|y)\n",
    "$$\n",
    "\n",
    "In other words, once the class $y$ is known, the probability of a word appearing in the email is assumed to be independent of the other words. While this assumption is not true, it can be sufficiently accurate to yield effective predictions.\n",
    "\n",
    "Combining these components, the classification rule becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max} \\; \\prod_{j=1}^n p(x_j|y) \\, p(y)\n",
    "$$\n",
    "\n",
    "To avoid numerical underflow and to simplify computations, it is common to perform the calculation in log space:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max} \\; \\sum_{j=1}^n \\log p(x_j|y) + \\log p(y)\n",
    "$$\n",
    "\n",
    "This formulation highlights the distinction between **generative** and **discriminative** models. A generative model like Naive Bayes makes use of the likelihood term, expressing how the features of an email are generated given its class (spam or ham). A discriminative model, in contrast, focuses directly on estimating $p(y|\\mathbf{x})$, potentially by assigning weights to features that maximize classification accuracy without modeling how the data is generated.\n",
    "\n",
    "#### Estimation\n",
    "\n",
    "We now turn to the estimation of parameters for the Bernoulli Naive Bayes model. The key parameters are defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_{(j\\mid y=1)} &= p(x_j=1\\mid y=1) \\\\ \n",
    "\\phi_{(j\\mid y=0)} &= p(x_j=1\\mid y=0) \\\\ \n",
    "\\phi_{(y)} &= p(y=1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "These parameters can be interpreted as:\n",
    "\n",
    "- $\\phi_{(j\\mid y=1)}$: the probability that word $j$ appears in an email, given the email is spam.  \n",
    "- $\\phi_{(j\\mid y=0)}$: the probability that word $j$ appears in an email, given the email is not spam.  \n",
    "- $\\phi_{(y)}$: the prior probability that a randomly chosen email is spam.  \n",
    "\n",
    "To estimate these parameters, we maximize the **joint likelihood**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_{(y)}, \\phi_{(j\\mid y)}) = \\prod_{i=1}^{m} p(\\mathbf{x}^{(i)}, y^{(i)}; \\phi_{(y)}, \\phi_{(j\\mid y)}).\n",
    "$$\n",
    "\n",
    "The standard approach is **maximum likelihood estimation (MLE)**. Before deriving the results formally, we can already anticipate the following intuitive estimates:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_{(y)} &= \\frac{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\}}{m} \\\\[6pt]\n",
    "\\phi_{(j\\mid y=1)} &= \\frac{\\sum_{i=1}^{m} 1\\{x_j^{(i)}=1,\\, y^{(i)}=1\\}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\}} \\\\[6pt]\n",
    "\\phi_{(j\\mid y=0)} &= \\frac{\\sum_{i=1}^{m} 1\\{x_j^{(i)}=1,\\, y^{(i)}=0\\}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=0\\}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "These formulas have straightforward interpretations:\n",
    "\n",
    "- $\\phi_{(y)}$: the fraction of spam emails in the dataset.  \n",
    "- $\\phi_{(j\\mid y=1)}$: among all spam emails, the fraction containing word $j$.  \n",
    "- $\\phi_{(j\\mid y=0)}$: among all non-spam emails, the fraction containing word $j$.  \n",
    "\n",
    "##### Derivation\n",
    "\n",
    "The likelihood can be factored as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_{(y)}, \\phi_{(j\\mid y)}) = \\prod_{i=1}^{m} p(y^{(i)};\\phi_{(y)}) \\, p(\\mathbf{x}^{(i)} \\mid y^{(i)};\\phi_{(j\\mid y)}).\n",
    "$$\n",
    "\n",
    "For $y^{(i)} \\in \\{0,1\\}$, the distribution of $y$ is Bernoulli:\n",
    "\n",
    "$$\n",
    "p(y^{(i)};\\phi_y)=\\phi_y^{y^{(i)}}(1-\\phi_y)^{1-y^{(i)}}.\n",
    "$$\n",
    "\n",
    "For a single feature $x_j^{(i)}$:\n",
    "\n",
    "$$\n",
    "p(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y})=\n",
    "\\begin{cases}\n",
    "\\phi_{j|1}^{\\,x_j^{(i)}}(1-\\phi_{j|1})^{1-x_j^{(i)}}, & \\text{if }y^{(i)}=1,\\\\[6pt]\n",
    "\\phi_{j|0}^{\\,x_j^{(i)}}(1-\\phi_{j|0})^{1-x_j^{(i)}}, & \\text{if }y^{(i)}=0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can be written more compactly using indicator functions, which activate the left or the right part of the equation depending on the current $y^{(i)}$:\n",
    "\n",
    "$$\n",
    "p(x_j^{(i)}\\mid y^{(i)}) =\n",
    "\\big(\\phi_{j|1}^{\\,x_j^{(i)}}(1-\\phi_{j|1})^{1-x_j^{(i)}}\\big)^{1\\{y^{(i)}=1\\}}\n",
    "\\big(\\phi_{j|0}^{\\,x_j^{(i)}}(1-\\phi_{j|0})^{1-x_j^{(i)}}\\big)^{1\\{y^{(i)}=0\\}}.\n",
    "$$\n",
    "\n",
    "The full joint likelihood becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Phi)\n",
    "=\\prod_{i=1}^m \\Big[\n",
    "\\phi_y^{y^{(i)}}(1-\\phi_y)^{1-y^{(i)}}\n",
    "\\prod_{j=1}^n \n",
    "\\big(\\phi_{j|1}^{\\,x_j^{(i)}}(1-\\phi_{j|1})^{1-x_j^{(i)}}\\big)^{1\\{y^{(i)}=1\\}}\n",
    "\\big(\\phi_{j|0}^{\\,x_j^{(i)}}(1-\\phi_{j|0})^{1-x_j^{(i)}}\\big)^{1\\{y^{(i)}=0\\}}\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "Taking the log yields:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Phi)\n",
    "= \\sum_{i=1}^m \\Big[ y^{(i)}\\log\\phi_y + (1-y^{(i)})\\log(1-\\phi_y)\\Big]\n",
    "+ \\sum_{j=1}^n \\sum_{i=1}^m \\Bigg\\{\n",
    "\\begin{aligned}\n",
    "&1\\{y^{(i)}=1\\}\\big[x_j^{(i)}\\log\\phi_{j|1} + (1-x_j^{(i)})\\log(1-\\phi_{j|1})\\big] \\\\\n",
    "&+\\,1\\{y^{(i)}=0\\}\\big[x_j^{(i)}\\log\\phi_{j|0} + (1-x_j^{(i)})\\log(1-\\phi_{j|0})\\big]\n",
    "\\end{aligned}\n",
    "\\Bigg\\}.\n",
    "$$\n",
    "\n",
    "##### Useful counts\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "N_1 \\;=\\; \\sum_{i=1}^m 1\\{y^{(i)}=1\\},\\qquad N_0 = m-N_1 = \\sum_{i=1}^m 1\\{y^{(i)}=0\\}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{j,1} \\;=\\; \\sum_{i=1}^m 1\\{y^{(i)}=1\\}\\,x_j^{(i)} \\;=\\; \\sum_{i=1}^m 1\\{x_j^{(i)}=1,\\;y^{(i)}=1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{j,0} \\;=\\; \\sum_{i=1}^m 1\\{y^{(i)}=0\\}\\,x_j^{(i)} \\;=\\; \\sum_{i=1}^m 1\\{x_j^{(i)}=1,\\;y^{(i)}=0\\}.\n",
    "$$\n",
    "\n",
    "With these, the log-likelihood simplifies into separate parts:\n",
    "\n",
    "- For $\\phi_y$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_y) = N_1\\log\\phi_y + (m-N_1)\\log(1-\\phi_y).\n",
    "$$\n",
    "\n",
    "- For $\\phi_{j|1}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_{j|1}) = S_{j,1}\\log\\phi_{j|1} + (N_1-S_{j,1})\\log(1-\\phi_{j|1}).\n",
    "$$\n",
    "\n",
    "- For $\\phi_{j|0}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi_{j|0}) = S_{j,0}\\log\\phi_{j|0} + (N_0-S_{j,0})\\log(1-\\phi_{j|0}).\n",
    "$$\n",
    "\n",
    "\n",
    "##### Maximization\n",
    "\n",
    "Differentiating and solving for $\\phi_y$ (first example in detail). Note the application of the chain rule and resulting changing of the signs:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\phi_y} = N_1 \\frac{1}{\\phi_y} * 1 + m-N_1 \\frac{1}{1-\\phi_y} * (-1) = \\frac{N_1}{\\phi_y} - \\frac{m-N_1}{1-\\phi_y}=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{N_1}{\\phi_y} &= \\frac{m-N_1}{1-\\phi_y} \\\\\n",
    "N_1(1-\\phi_y) &= (m-N_1)\\phi_y \\\\\n",
    "N_1 - N_1\\phi_y &= m\\phi_y - N_1\\phi_y \\\\\n",
    "N_1 &= m\\phi_y \\\\\n",
    "\\phi_y &= \\frac{N_1}{m} = \\frac{\\sum_{i=1}^m 1\\{y^{(i)}=1\\}}{m}.\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "For $\\phi_{j|1}$:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\phi_{j|1}} = \\frac{S_{j,1}}{\\phi_{j|1}} - \\frac{N_1-S_{j,1}}{1-\\phi_{j|1}} = 0\n",
    "\\quad\\Rightarrow\\quad \\phi_{j|1}=\\frac{S_{j,1}}{N_1} = \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)}=1, y^{(i)}=1\\}}{\\sum_{i=1}^m 1\\{y^{(i)}=1\\}}.\n",
    "$$\n",
    "\n",
    "For $\\phi_{j|0}$:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\phi_{j|0}} = \\frac{S_{j,0}}{\\phi_{j|0}} - \\frac{N_0-S_{j,0}}{1-\\phi_{j|0}} = 0\n",
    "\\quad\\Rightarrow\\quad \\phi_{j|0}=\\frac{S_{j,0}}{N_0} = \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)}=1, y^{(i)}=0\\}}{\\sum_{i=1}^m 1\\{y^{(i)}=0\\}}.\n",
    "$$\n",
    "\n",
    "In summary, the maximum likelihood estimates of the parameters coincide with the intuitive fractions derived earlier. They represent relative frequencies of word occurrences within spam and non-spam classes, along with the overall proportion of spam in the dataset.\n",
    "\n",
    "#### Laplace Smoothing\n",
    "\n",
    "Up to this point, the parameter estimation procedure for Bernoulli Naive Bayes is almost complete, but there is a crucial issue to address.  \n",
    "\n",
    "Consider the case where a particular word never appears in emails of one class (e.g., spam). This would cause the numerator of the estimate to be $0$, resulting in the parameter estimate for that feature also being $0$. Consequently, the likelihood (and therefore the posterior) would be $0$ as well. In practice, this means that if a word like *“chicken”* never appears in spam emails during training, then any email containing *“chicken”* would automatically be classified as non-spam, regardless of the other features.  \n",
    "\n",
    "Beyond corrupting predictions, this also introduces a computational issue: taking the logarithm of $0$ is undefined, which can cause errors during implementation.  \n",
    "\n",
    "From a logical standpoint, the problem arises because the absence of an event in the training set does not imply that the event has zero probability of occurring in the future. This is known as the **zero-frequency problem**.  \n",
    "\n",
    "A standard solution is to apply a smoothing technique. For Bernoulli Naive Bayes (and categorical Naive Bayes in general), **Laplace smoothing** is commonly used. The adjusted estimate is:\n",
    "\n",
    "$$\n",
    "\\phi_{(j \\mid y=1)} = \\frac{\\sum_{i=1}^{m} 1\\{x_{j}^{(i)}=1, y^{(i)}=1\\} + l}{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\} + lK}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $l$ is the smoothing parameter,  \n",
    "- $K$ is the number of feature classes (not label classes).  \n",
    "\n",
    "If $l = 0$, we recover the maximum likelihood estimator (MLE). Setting $l = 1$ corresponds to Laplace smoothing.  \n",
    "\n",
    "The same adjustment is applied to $\\phi_{(j \\mid y=0)}$.  \n",
    "In the case of Bernoulli Naive Bayes with binary features, this means adding $1$ to the numerator and $2$ to the denominator.\n",
    "\n",
    "##### Example of Laplace Smoothing\n",
    "\n",
    "Suppose we are estimating the probability that the word *“chicken”* appears in a spam email.  \n",
    "\n",
    "- In the training set, we have $m = 100$ emails labeled as spam.  \n",
    "- The word *“chicken”* never appears in any of these spam emails.  \n",
    "\n",
    "Without smoothing, the maximum likelihood estimate would be:\n",
    "\n",
    "$$\n",
    "\\phi_{(\\text{chicken} \\mid y=1)} = \\frac{0}{100} = 0\n",
    "$$\n",
    "\n",
    "This would lead to the zero-frequency problem described earlier.\n",
    "\n",
    "Now let us apply **Laplace smoothing** with $l=1$ and $K=2$ (since the feature is binary: present or absent). The formula becomes:\n",
    "\n",
    "$$\n",
    "\\phi_{(\\text{chicken} \\mid y=1)} = \\frac{0 + 1}{100 + 2} = \\frac{1}{102} \\approx 0.0098\n",
    "$$\n",
    "\n",
    "So instead of assigning zero probability, we assign a small but nonzero probability.  \n",
    "\n",
    "This adjustment ensures that the model does not automatically rule out spam emails containing *“chicken”*, even though the training data did not include such examples.\n",
    "\n",
    "#### Training\n",
    "\n",
    "The parameter estimation procedures discussed earlier are carried out during the **training phase** using the training dataset.  \n",
    "\n",
    "This process differs significantly from models such as logistic regression. In logistic regression, training involves iterative optimization methods (e.g., gradient descent) to fit the parameters. By contrast, the Naive Bayes algorithm estimates each parameter independently through straightforward counting procedures.  \n",
    "\n",
    "This non-iterative process makes Naive Bayes computationally very efficient, which is one of its key advantages.\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "The goal of the Naive Bayes classifier in the prediction phase is to assign a new observation to one of the possible classes. In the spam detection example, this means classifying an email as either spam or non-spam.  \n",
    "\n",
    "The decision rule is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max} \\; \\sum_{j=1}^n \\log p(x_j \\mid y) + \\log p(y)\n",
    "$$\n",
    "\n",
    "During prediction, we replace the probabilities with the parameter estimates obtained in the training phase:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max} \\; \\sum_{j=1}^n \\Bigl(x_j \\log \\phi_{j \\mid y} + (1-x_j)\\log(1-\\phi_{j \\mid y})\\Bigr) + \\log \\phi_y\n",
    "$$\n",
    "\n",
    "Breaking this down, we compute the **log-scores** for each class:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_1 &= \\sum_{j=1}^n \\Bigl(x_j\\log\\phi_{j \\mid 1} + (1-x_j)\\log(1-\\phi_{j \\mid 1})\\Bigr) + \\log \\phi_y \\\\\n",
    "s_0 &= \\sum_{j=1}^n \\Bigl(x_j\\log\\phi_{j \\mid 0} + (1-x_j)\\log(1-\\phi_{j \\mid 0})\\Bigr) + \\log (1 - \\phi_y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The predicted label $\\hat{y}$ is the class corresponding to the larger score.  \n",
    "\n",
    "If we want to recover the actual **posterior probabilities** for the email to be a spam or non-spam, we must normalize the scores:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y=1 \\mid \\mathbf{x}) &= \\frac{e^{s_1}}{e^{s_0} + e^{s_1}} = \\frac{1}{1 + (e^{s_0} - e^{s_1})}\\\\\n",
    "p(y=0 \\mid \\mathbf{x}) &= 1 - p(y=1 \\mid \\mathbf{x})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This normalization step is equivalent to reintroducing $p(\\mathbf{x})$ into Bayes’ theorem.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "As with most classification problems, we evaluate the performance of the Naive Bayes classifier using the standard set of metrics, which were discussed in more detail in the previous notebooks: **Accuracy**, **Precision**, **Recall**, **F1 Score**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ef2f4",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes Implemention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f302a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb9899",
   "metadata": {},
   "source": [
    "We begin by loading the SMS Spam Collection dataset, which contains messages labeled as either spam or ham (non-spam). This dataset serves as a practical example for the Bernoulli Naive Bayes classifier we discussed in the theory section. Each message is represented as a text string, and the corresponding label indicates whether it is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../datasets/sms_spam/SMSSpamCollection.txt\"\n",
    "spam = pd.read_csv(data_path,delimiter=\"\\t\",header=None)\n",
    "spam.columns = ['class', 'message']\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6870",
   "metadata": {},
   "source": [
    "This section performs preprocessing on the raw SMS messages to prepare them for modeling. The steps include lowercasing, replacing emails, URLs, phone numbers, currency symbols, and numbers with placeholder tokens, removing punctuation, tokenizing, correcting spelling errors, and stemming words while removing stop words.  \n",
    "\n",
    "In this context, **stemming** means reducing words to their root form (e.g., \"running\" becomes \"run\") to consolidate similar word forms. Both a stemmed and a non-stemmed version of each message is stored.  \n",
    "\n",
    "**Notes:**  \n",
    "- The `nltk.download('stopwords')` command only needs to be run once per system.  \n",
    "- The entire preprocessing step can be skipped if you prefer to directly use the previously cleaned dataset saved as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrected(msg):\n",
    "  spell = SpellChecker()\n",
    "\n",
    "  # find those words that may be misspelled\n",
    "  misspelled = spell.unknown(msg)\n",
    "  corrected_words = {}\n",
    "  for word in misspelled:\n",
    "      # Get the one `most likely` answer\n",
    "      correct_spell = spell.correction(word)\n",
    "      if correct_spell != word:\n",
    "          corrected_words[word] = correct_spell\n",
    "\n",
    "  for m in range(len(msg)):\n",
    "    try:\n",
    "      if corrected_words[msg[m]]:\n",
    "        msg[m] = corrected_words[msg[m]]\n",
    "    except Exception:\n",
    "      pass\n",
    "  return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_stemmed = []\n",
    "corpus_not_stemmed = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(spam['message'])):\n",
    "    # Applying Regular Expression\n",
    "\n",
    "    # Replace email addresses with 'emailaddr'\n",
    "    # Replace URLs with 'httpaddr'\n",
    "    # Replace money symbols with 'moneysymb'\n",
    "    # Replace phone numbers with 'phonenumbr'\n",
    "    # Replace numbers with 'numbr'\n",
    "      \n",
    "    msg = spam['message'][i]\n",
    "\n",
    "    # Each word to lower case\n",
    "    msg = msg.lower()\n",
    "    msg = re.sub(r'[\\w\\-.]+?@\\w+?\\.\\w{2,4}', ' emailaddr ', msg)\n",
    "    msg = re.sub(r'(http[s]?\\S+)|(\\w+\\.[a-z]{2,4}\\S*)', 'httpaddr', msg)\n",
    "    msg = re.sub(r'£|\\$', ' moneysymb ', msg)\n",
    "    msg = re.sub(r' [0-9]{4}(-)?[0-9]{3}(-)?[0-9]{4} ', ' phonenumber ', msg)\n",
    "    msg = re.sub(r'\\d+(\\.\\d+)?', ' number ', msg)\n",
    "    msg = re.sub(r' u ', ' you ', msg)\n",
    "\n",
    "    # Remove all punctuations\n",
    "    msg = re.sub(r'[^\\w\\d\\s]', ' ', msg)\n",
    "    # Splitting words to Tokenize\n",
    "    msg = msg.split()\n",
    "    msg = spell_corrected(msg)\n",
    "    corpus_not_stemmed.append(' '.join(msg))\n",
    "    # Stemming with PorterStemmer handling Stop Words\n",
    "    msg = [ps.stem(word) for word in msg if not word in set(stopwords.words('english'))]\n",
    "    # Preparing Messages with Remaining Tokens\n",
    "    msg = ' '.join(msg)\n",
    "    # Preparing WordVector Corpus\n",
    "    corpus_stemmed.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de24c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['stemmed'] = corpus_stemmed\n",
    "spam['not_stemmed'] = corpus_not_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eff09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.to_csv(\"../datasets/sms_spam/sms_spam_cleaned.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def58d3",
   "metadata": {},
   "source": [
    "Here we load the previously cleaned SMS spam dataset from the CSV file. After inspecting the first few rows and general dataset information, any missing entries in the stemmed or non-stemmed message columns are replaced with empty strings. Finally, the distribution of classes is printed to examine the proportion of spam versus non-spam messages in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d66fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "class",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "message",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stemmed",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "not_stemmed",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6182a426-5028-483e-a908-8bd7fc68a6e3",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "go wrong point crazi avail bug n great world la e buffet cine got wat",
         "go until wrong point crazy available only in bugs n great world la e buffet cine there got more wat"
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni...",
         "ok lar joke",
         "ok lar joking if you on"
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "free entri number wili come win fa cup final tit number st may number text fa number receiv entri question sad text rate c appli number number",
         "free entry in number a wily come to win fa cup final tits number st may number text fa to number to receive entry question sad text rate t c s apply number over number s"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say...",
         "u dun say earli c alreadi say",
         "u dun say so early for you c already then say"
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though",
         "nah think goe us live around though",
         "nah i don t think he goes to us he lives around here though"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>not_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go wrong point crazi avail bug n great world l...</td>\n",
       "      <td>go until wrong point crazy available only in b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke</td>\n",
       "      <td>ok lar joking if you on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri number wili come win fa cup final t...</td>\n",
       "      <td>free entry in number a wily come to win fa cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli c alreadi say</td>\n",
       "      <td>u dun say so early for you c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goe us live around though</td>\n",
       "      <td>nah i don t think he goes to us he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  go wrong point crazi avail bug n great world l...   \n",
       "1                                        ok lar joke   \n",
       "2  free entri number wili come win fa cup final t...   \n",
       "3                      u dun say earli c alreadi say   \n",
       "4                nah think goe us live around though   \n",
       "\n",
       "                                         not_stemmed  \n",
       "0  go until wrong point crazy available only in b...  \n",
       "1                            ok lar joking if you on  \n",
       "2  free entry in number a wily come to win fa cup...  \n",
       "3      u dun say so early for you c already then say  \n",
       "4  nah i don t think he goes to us he lives aroun...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv(\"../datasets/sms_spam/sms_spam_cleaned.csv\", delimiter=\";\", index_col=0)\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93100f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5572 entries, 0 to 5571\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   class        5572 non-null   object\n",
      " 1   message      5572 non-null   object\n",
      " 2   stemmed      5562 non-null   object\n",
      " 3   not_stemmed  5570 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 217.7+ KB\n"
     ]
    }
   ],
   "source": [
    "spam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1b655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['stemmed'] = spam['stemmed'].fillna(\"\")\n",
    "spam['not_stemmed'] = spam['not_stemmed'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0430d594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "class",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cbfdb8c5-ca39-4e76-a7cc-045c9cbefca4",
       "rows": [
        [
         "ham",
         "4825"
        ],
        [
         "spam",
         "747"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "class\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e24e16",
   "metadata": {},
   "source": [
    "In this step, we convert the preprocessed text messages into a numerical feature representation using `CountVectorizer`. Each unique word in the dataset becomes a feature (column), and each message is represented as a binary vector indicating the **presence (1) or absence (0)** of each word. This binary representation is suitable for the Bernoulli Naive Bayes classifier, which models features as being either present or absent.  \n",
    "\n",
    "After the transformation, the feature matrix `X` has shape `(5572, 4905)`, indicating there are 5,572 messages and 4,905 unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91568879",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "X = cv.fit_transform(spam['stemmed']).toarray()\n",
    "feat = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55e2e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4905)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b700f9",
   "metadata": {},
   "source": [
    "Here, the class labels are converted into a binary numerical format suitable for modeling. Messages labeled as **ham** (non-spam) are assigned a value of `0`, while **spam** messages are assigned a value of `1`. This creates the target vector `y` for training the Bernoulli Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582ded50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(spam['class'] == 'ham', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba424db",
   "metadata": {},
   "source": [
    "In this section, the dataset is first **shuffled** to randomize the order of the messages, which helps ensure that the training and test sets are representative and not biased by any ordering in the original data.  \n",
    "\n",
    "Next, the dataset is **split** into training and test sets according to a specified ratio (`split_ratio = 0.75`). This means 75% of the data is used for training the Bernoulli Naive Bayes model, and the remaining 25% is reserved for evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c01f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    shuffle_indices = np.random.permutation(len(X))\n",
    "    X, y = X[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e85a3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, split_ratio):\n",
    "    split_size = int(len(X) * split_ratio)\n",
    "    X_train = X[:split_size]\n",
    "    y_train = y[:split_size]\n",
    "    X_test = X[split_size:]\n",
    "    y_test = y[split_size:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736d08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdd989b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_dataset(X, y, split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f657c",
   "metadata": {},
   "source": [
    "This section implements a **Bernoulli Naive Bayes classifier from scratch**.  \n",
    "\n",
    "- The `fit` method estimates the parameters from the training data:  \n",
    "  - `likelihood_one` and `likelihood_zero` store the probability of each word being present given that the message is spam (`1`) or ham (`0`), applying Laplace smoothing.  \n",
    "  - `prior` stores the overall probability of a message being spam in the training set.  \n",
    "\n",
    "- The `predict` method computes the **log-joint likelihoods** for each class, adds the log prior, and assigns each message to the class with the higher score.  \n",
    "\n",
    "Finally, the model is instantiated and fitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37233332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:  \n",
    "    def fit(self, X, y):\n",
    "        self.likelihoods_1 = (np.sum(X[y == 1], axis=0) + 1) / (np.sum(y) + np.unique(y).size)\n",
    "        self.likelihoods_0 = (np.sum(X[y == 0], axis=0) + 1) / (len(y) - np.sum(y) + np.unique(y).size)\n",
    "        self.prior = np.sum(y) / len(y)\n",
    "   \n",
    "    def predict(self, X):\n",
    "        joint_log_likelihood_1 = np.sum(X * np.log(self.likelihoods_1) + (1 - X) * np.log(1 - self.likelihoods_1), axis=1)\n",
    "        joint_log_likelihood_0 = np.sum(X * np.log(self.likelihoods_0) + (1 - X) * np.log(1 - self.likelihoods_0), axis=1)\n",
    "        log_posterior_1 = joint_log_likelihood_1 + np.log(self.prior)\n",
    "        log_posterior_0 = joint_log_likelihood_0 + np.log(1 - self.prior)\n",
    "        y_pred = (log_posterior_1 > log_posterior_0).astype(int)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5900276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNaiveBayes()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2170aa",
   "metadata": {},
   "source": [
    "In this step, the fitted Bernoulli Naive Bayes model is used to **predict the class labels** of the test set. The predictions are then evaluated using standard classification metrics: precision, recall, F1-score, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aef251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7fd7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1205\n",
      "           1       0.97      0.89      0.93       188\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0954c2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented a Bernoulli Naive Bayes classifier from scratch and applied it to a simplified SMS spam detection example. While the model achieved high accuracy, this serves mainly as a demonstration; in a real-world scenario, additional exploratory data analysis, feature engineering, and model tuning would likely be necessary to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5d04b",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "Jurafsky, D., & Martin, J. H. (2020). *Speech and language processing* (Draft of January 20, 2020, Chapter 4: Naive Bayes and Sentiment Classification). Retrieved from https://web.stanford.edu/~jurafsky/slp3/old_dec20/ed3book_dec302020.pdf\n",
    "\n",
    "Weinberger, K. (2018). *Lecture note 05: Bayes Classifier and Naive Bayes*. Cornell University CS4780: Machine Learning for Intelligent Systems. Retrieved from https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html  \n",
    "\n",
    "Collins, M. (2012). *The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm*. Columbia University. Retrieved from https://www.cs.columbia.edu/~mcollins/em.pdf  \n",
    "\n",
    "Sanderson, G. (2019). *Bayes' theorem* [Video]. 3Blue1Brown. Retrieved from https://www.3blue1brown.com/lessons/bayes-theorem  \n",
    "\n",
    "Ng, A. (2018). *Lecture 5 - GDA & Naive Bayes* [Video]. YouTube. Stanford CS229: Machine Learning. Retrieved from https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=5\n",
    "\n",
    "Ng, A. (2018). *Lecture 6 - Support Vector Machines* [Video]. YouTube. Stanford CS229: Machine Learning. Retrieved from https://www.youtube.com/watch?v=lDwow4aOrtg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=6  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
